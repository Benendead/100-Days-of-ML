Day 1: July 5th, 2018
Today's Progress: I've moved from 75% to 85% done with Codecademy's introductory Python course. I finished up sections on
list/dictionary manipulation and also bitwise operations. I'm now a bit over halfway through the Classes section, which is a
welcome step up in complexity. I should be able to finish this course within the week, maybe tomorrow if I work at it.
Thoughts: I'm finally in the Classes segment of a coding course, so you know I'm getting up there. One particularly
interesting section of code they had me write was this bitwise operation method:

# This method flips a specified bit in a given number.
def flip_bit(number, n): 
  mask = 0b1 << (n-1) 
  result = mask ^ number 
  return bin(result)
  
Day 2: July 6th, 2018 (1 hour extra so far)
Today's Progress: I finished the Codecademy Python course this morning and then spent the rest of that first hour beginning
a linear algebra book. Tonight, in a second hour of work, I began reading Learning From Data before realizing that a more 
rigorous background in linear algebra is needed, and so I've now began the UTAustinX Linear Algebra - Foundations to 
Frontiers course on edX. My goal is to complete that course, supplifmented by the mentioned workbook, and then see how far I
can comprehend Learning From Data. Added note: I'm on 0.2.4 of the edX course, as it doesn't save my progress on its own.
Thoughts: It's frusturating to constantly need a more robust background in whatever math area is next, but I hope that by 
cracking down and working hard I can truly gain understanding of these fundamentals so that my ML endeavors are a breeze.

Day 3: July 7th, 2018 (1 cumulative hour extra)
Today's Progress: Downloaded and activated MATLAB 2018 using my school's license. Took quite a long time to download, so
really only accomplished that, installing XCode (for C compiling), and moving to 0.3.3 in the edX linear algebra (LA from
here on out) course.
Thoughts: Hopeful that I can get in some actual learning progress tomorrow, as opposed to figuring out licenses and just
downloading applications.

Day 4: July 8th, 2018 (1 cumulative hour extra)
Today's Progress: Set up Mathworks Online for the edX course, finished up through 1.2.2 of the course. So far have taken 
almost a page of notes; I'm now on 1.3.1 for tomorrow.
Thoughts: Glad to start learning some notation. The course seems well presented and is clear about the notation; I've already
learned a few things that I hadn't previously encountered in multivariable calculus.

Day 5: July 9th, 2018 (1 cumulative hour extra)
Today's Progress: Moved from edX 1.3.1 to halfway through 1.4.1. I learned basic vector operations, some proof writing skills,
and the intuition for calculating the cost of these simple operations in terms of flops and memops.
Thoughts: About time that I get to do some math/proofs with this course. The flops/memops part reminds me of Big-O notation,
which we touched on in AP Comp Sci, although I'm sure this will evolve into a more rigorous part of the course eventually.
For the sake of honesty, these hours are getting more difficult as I have busier and busier days and need to push these back
late into the night, but I hope that I can do this earlier in the day in the coming week as to avoid dreading this learning.

Days 6 and 7: July 11th, 2018 (2 cumulative hours extra)
Today's Progress: Yesterday, I watched one of Daniel Shiffman's videos on perceptrons. I then attempted to follow along with
his code in Processing, but my laptop couldn't handle the app for whatever reason. Today, I installed Jupyter Notebooks and
developed a perceptron model as my first Python project. It's near completion and I have a ton of ideas for what experiments
to run using it. I did also do at least an hour extra of work today working on this perceptron project.
Thoughts: Python is definitely the way to go, especially when my laptop only has 4GB of RAM. I'm excited to polish up my 
perceptron model and publish it to Github, although I'm certain that there's a lot I could improve on it.

Day 8: July 12th, 2018 (2 cumulative hours extra)
Today's Progress: I added a test method and visualization to the Perceptron project, created more clear training methods,
given the user the option to separate their data into training and test data, and I've begun varying the example number and
allowed training steps to attempt to check that the model can make errors and isn't cheating in some way. I imagine it'll be
ready to release for proofreading sometime tomorrow.
Thoughts: I'd like to make sure my code is crystal clear before I release it and there're glaring holes in my explanations or
code documentation. I should probably learn proper Python documentation formatting.

Day 9: July 13th, 2018 (2 cumulative hours extra)
Today's Progress: Studied Python documentation best practices and then went through my perceptron project and reformatted
all of the class/method comments to proper form. Want to proofread through one more time before publishing.
Thoughts: I'm feeling pretty good about my project now that it's well documented. It creates a sense of legitimacy in
knowing that my code's a lot closer to professional now. Might want to pursue another Python course to round out my 
knowledge in areas of unknown deficiencies.

Days 10 and 11: July 15th, 2018 (1 cumulative hour extra)
Yesterday's Progress: I was busy, had to work, and was unable to study ML yesterday. Good thing I'm working extra hours 
ahead of time.
Today's Progress: I completed the edX course on linear algebra through 1.4.4.6. Still need to finish up the unit, but I 
did work through two proofs today on my own and I'm continuing to improve on the format.
Thoughts: I can't miss another day; it's much more difficult to jump back into linear algebra because it's a new topic for me.

Day 12: July 16th, 2018 (1 cumulative hour extra)
Today's Progress: I actually accumulated a bunch of resources on music/composition/transcription and machine learning, and
then read through one article described a method for transcribing MIDI piano using a CNN. I learned a few terms, got some
intuition, but of course there's a lot more to look up and explore. 
Thoughts: This type of learning process is so much more fun than the dry linear algebra course, which isn't scheduled
as a real course right now anyways, so I may learn like this for a bit. Definitely seeing a ton more applications of my
interest than I was scrolling through vector proofs.

Day 13: July 17th, 2018 (2 cumulative hours extra)
Today's Progress: Read most of an explanatory article on CNNs. Took my own notes, defining terms along the way, and I think
I'm building a fair intuition into the network as a whole.
Thoughts: I should probably buckle down and commit to a beginning-to-end ML book if I'm going to be studying the hard-hitters
anyways. Might crack open my Deep Learning (Goodfellow) book today, try to move through more of the math in the beginning.
Also: I studied another hour, finishing up the ConvNet article and beginning one on using LSTMs/RNNs to compose music.

Days 14 and 15: July 19th, 2018 (1.5 cumulative hours extra)
Yesterday: Missed a day, as I was ironically up so late the night before that I'd completed that extra hour fully in this day.
Today's Progress: I spent a half hour watching and taking notes on two carykh videos, noting his sources and where I can learn
more about his hyperGAN and CNN shenanigans. He makes concepts lucid but doesn't go much in depth, unfortunately. Tonight I
also spent another hour continuing through Daniel Johnson's RNN/LSTM composition project, which got really dense and hence
took me a while to slog through. I believe I can finish the original article with another hour of study.
Thoughts: I should go back and proofread through my perceptron class and publish it, only because I feel as though I may
forget otherwise. It's competent, surely.

Day 16: July 20th, 2018 (1.5 cumulative hours extra)
Today's Progress: Spent the hour debugging, proofreading, and testing the perceptron project. Then released it on Github.
Thoughts: Given that my one completed project was based on a video by Daniel Shiffman, I think that continuing with his
resources for inspiration/learning might be a wise choice.

Day 17: July 21st, 2018 (1.5 cumulative hours extra)
Today's Progress: Watched through Daniel Shiffman's second video on his perceptron model, where he refactored his code and
also added a visualization of the perceptron's current guess of the line. I implemented these changes into my project as well.
Thoughts: Daniel Shiffman's videos are indeed both educational and motivating, I'll keep going through them and then might
start through Siraj's. Good progress and my project is on Github still check it out.

Days 18 and 19: July 23rd, 2018 (1.5 cumulative hours extra)
Yesterday's Progress: Studied another two Daniel Shiffman videos before reaching linear algebra that deserves more dedication,
although perhaps that's my problem of not progressing until every foundation is completely robust. Then spent 20min (still
part of the hour) taking notes on the Deep Learning book by Goodfellow, still in the math section.
Today's Progress: Studied through most of the first unit of a Microsoft edX course called "Essential Math for Machine 
Learning: Python Edition." It's quite basic so far but finishing all of this would (in my mind) legitimize starting other
courses without worrying about all of the math I'm still missing.
Thoughts: Maybe five more days of this Microsoft course and it'll be done at this rate; of course I expect it to pick up
difficulty but we'll see. I also need some pandas and matplotlib literacy work, as they're currently incomprehensible.

Days 20 and 21: July 25th, 2018 (1.5 cumulative hours extra)
Today and Yesterday's Progress: Yesterday I took a Boston University multivariable calculus final exam in order to practice
for a placement test I'll need to take at UR in a few weeks. I got 45/100 on this, which isn't terrible, corrected the test
between yesterday and today, and am now studying the topics in need of review. For example, I studied line integrals as well
as directional derivatives today.
Thoughts: Yes, multivariable is only tangentially related to ML, but passing this test is probably the single most effective
course of action I can pursue in order to accelerate my ML studies in college and beyond. I do plan to finish the other math
edX course as well, simply because it seems both quick and relatively broad in content and hence learning.

Day 22: July 26th, 2018 (1.5 cumulative hours extra)
Today's Progress: I spent half of my hour organizing new sources and then reading an article on the best ways to begin
reading ML research papers. I then finished the first module of the edX course on math for ML, which was easy yet satisfying.
Thoughts: The edX course only gets more difficult from here, so hopefully I can continue along at a similarly quick pace
while still understanding everything as well as I have so far. Still motivated to learn in a lot of ways and I think putting
together my planned pathway forward would be motivational and useful:

    PATH FORWARD
      1 - Finish "Essential Math for Machine Learning: Python Edition" edX course.
      2 - Complete Daniel Shiffman's videos on Neural Networks.
        2.5 - Code his examples in Python, studying up on numpy, matplotlib, or pandas if necessary.
      3 - Read through the Neural Networks/Deep Learning book online.
      4 - Read a full list of groundbreaking/essential ML research articles.
        4.5 - Attempt to recreate important algorithms and replicate the papers' results.
      5 - Study and synthesize existing compositional ML works into my own original generative music network.

    Bonus: Continue through Deep Learning textbook, continue to study Multivariable Calculus.

Day 23: July 27th, 2018 (1.5 cumulative hours extra)
Today's Progress: Studied multivariable for the duration of my hour by finishing an assignment (assigned by me) on line
integrals and then beginning to study the fundamental theorem of line integrals. Point is that I should finish up this last
chapter of multivariable review, as it was difficult/rushed during the school year, and then continue on with ML research
as previously scheduled.
Thoughts: Multivariable is extremely satisfying; I dread returning to the drab world of simpler math in the edX course.

Day 24: July 28th, 2018 (2 cumulative hours extra)
Today's Progress: Studied multivariable again, this time path independence and line integrals in conservative vector fields.
I used Khan Academy's videos on the topic and will probably return to my textbook for exercises tomorrow.
Thoughts: Although this for once is not easy to re-learn, it's rewarding to develop those few-and-far-between multivariable
weaknesses that I had coming out of last school year. I definitely think that passing this test is feasible.

Days 25 and 26: July 30th, 2018 (3 cumulative hours extra)
Today's progress: Yesterday and today I spent an hour each day studying multivariable calculus. I'm now on Green's Theorem
with only about four chapters left in the "vector calculus" final chapter of my textbook. I also spent an hour today on the
edX "Math for ML" course and have finished the calculus section. Next up is vectors and matrices, so I'll finally be learning
something at least. Definitely going faster than the other LA course was.
Thoughts: Given that I've been catching up in MV Calc and advancing in my ML path, I'm hopeful that I can begin coding my own
models again within a few days. Might double up hours tomorrow as well; we'll see.

Day 27: July 31st, 2018 (5 cumulative hours extra, multivariable to this point totals 7.5 hours)
Today's Progress: Studied multivariable for an hour, completing a section on Green's Theorem and beginning one on curl and
divergence. Then spent two hours on the edX "Math for ML" course, making strong progress through the linear algebra section.
Also spent part of the last hour working through the Deep Learning book's linear algebra section.
Thoughts: Having seen some people's Tweets about this 100-day challenge, I feel as though I'm being ineffective as well as a
bit unfair with counting multivariable studying as ML research/coding. As such, I'd like to work more hours on ML in order to
replace the hours I've studied multivariable so far with pure ML research time. This process will take time but I can save
the current progress in the cumulative section each day.

Day 28: August 1st, 2018 (5 cumulative hours extra, multivariable to this point totals 7.5 hours)
Today's Progress: Read Adit Deshpande's second introductory article to CNNs, which was again quite excellent in explanation
and readability. I'll finish it up tomorrow and I'd also like to continue through his other guides and series.
Thoughts: He has one Github repository on ML with tons of resources and "lessons learned," which to me seems like an excellent
course of action once I'm between Daniel Shiffman's videos and reading additional ML papers. I would truly like to be able to
reach the level of involvement and comprehension he seems to have in the field in the next three years.

Day 29: August 2nd, 2018 (6.5 cumulative hours extra, multivariable to this point totals 8.5 hours)
Today's Progress: Curl/divergence on MV, began the "Deep Dive 1" project where I'll be taking and reading every source from
a given paper in chronological order. I've chosen the ImageNet-winning 2012 paper by Alex Krizhevsky. I have all 26 sources
documented, PDFs uploaded to GoogleDocs, and the links to where I got the PDFs. I need to order the papers chronologically
and then can begin reading through what will be a long process.
Thoughts: 1.5 hours of filing is a bit boring, but I think it's worth it to have a system set up for success. It's also less
draining to have a break from the constant learning, even for one day.

Days 30 and 31: August 4th, 2018 (5.5 cumulative hours extra, multivariable to this point totals 8.5 hours)
Yesterday I missed productive work; I was distracted by a potential future project yet failed to adequately commit to 
documenting the idea. For future reference: Something involving CycleGANs and deep-frying memes.
Today's Progress: Read 5.5 pages of the 1990 paper "Handwritten Digit Recognition with a Back-Propagation Network." It's
clear, concise, and I'm already in the Results section. Only issue is that it doesn't give/explain its back-propagation
algorithm, and that's kind of what I'm trying to still learn.
Thoughts: It's rewarding and yet relaxing to read a research paper that was once pivotal for its time and fully comprehend
almost every single sentence on the first or second read. I'd like to pursue a few tangential questions the paper brings up
to me, but beyond that I think recursive research of sources is just asking for trouble.
